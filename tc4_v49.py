# -*- coding: utf-8 -*-
"""TC4_v49.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EK22ucNZ6peJPpvYLBPeQ5ZmO7NJ6TWf

# üåü **Tech Challenge 4**

O **Tech Challenge** desta fase ser√° a cria√ß√£o de uma aplica√ß√£o que utilize **an√°lise de v√≠deo**.  

O projeto deve incorporar as seguintes t√©cnicas:  
üéØ **Reconhecimento facial**  
üé≠ **An√°lise de express√µes emocionais**  
üèÉ‚Äç‚ôÇÔ∏è **Detec√ß√£o de atividades em v√≠deos**  
üìä **Gera√ß√£o de resumo autom√°tico**

---

## üöÄ **A Proposta do Desafio**  

Voc√™ dever√° criar uma aplica√ß√£o a partir do v√≠deo dispon√≠vel na plataforma do aluno, que execute as seguintes tarefas:  

### 1. **üë§ Reconhecimento Facial**  
   - Identifique e marque os rostos presentes no v√≠deo.  

### 2. **üòä An√°lise de Express√µes Emocionais**  
   - Analise as express√µes emocionais dos rostos identificados.  

### 3. **üï∫ Detec√ß√£o de Atividades**  
   - Detecte e categorize as atividades sendo realizadas no v√≠deo.  

   **Atividades a serem detectadas:**  
   - ‚úã **M√£o esquerda levantada**  
   - ‚úã **M√£o direita levantada**  
   - üôå **M√£os levantadas**  
   - üíÉ **Dan√ßando**  
   - ü§ù **Aperto de m√£os**  

### 4. **üìÑ Gera√ß√£o de Resumo**  
   - Crie um resumo autom√°tico das principais atividades e emo√ß√µes detectadas no v√≠deo.  
   - Relat√≥rio gerado em formato `.txt` e exibido na tela com √≠cones e informa√ß√µes detalhadas.

---
"""

# 1. Instalar DeepFace e depend√™ncias essenciais
!pip install deepface opencv-python-headless retina-face

# 2. Atualizar pandas, tqdm, matplotlib para garantir compatibilidade
!pip install --upgrade pandas tqdm matplotlib

!pip install retina-face

!pip install mediapipe

!pip install deepface mediapipe moviepy tqdm opencv-python-headless

!pip install mtcnn

!pip install fer

!pip install tensorflow

#from google.colab import drive
#drive.mount('/content/drive')

!pip install tensorflow==2.9.1 keras==2.9.0
!pip install --upgrade deepface opencv-python-headless retina-face

"""Ajustando Reconhecimento Facial para precis√£o de evento √∫nicos, sem pular Frames"""

!pip install --upgrade tensorflow deepface

!pip install tensorflow keras opencv-python-headless deepface

"""## 1. üë§ Reconhecimento Facial"""

import cv2
from deepface import DeepFace
from tqdm import tqdm
import os

# Configurando pasta de sa√≠da
output_dir = '/content/drive/MyDrive/Analise_Emocoes_TC4'
os.makedirs(output_dir, exist_ok=True)

# Fun√ß√£o principal para reconhecimento facial no v√≠deo
def process_video_faces_only(video_path):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_fps = cap.get(cv2.CAP_PROP_FPS)
    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    output_video_path = os.path.join(output_dir, 'analisado35e_video_faces_only.mp4')
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, original_fps, (original_width, original_height))

    unique_people_count = 0
    detected_people_history = set()  # Hist√≥rico de pessoas detectadas
    frame_threshold = 10  # Limite de frames para considerar uma nova detec√ß√£o
    position_threshold = 10  # Limite de varia√ß√£o de posi√ß√£o para considerar o mesmo rosto

    with tqdm(total=total_frames, desc="Analisando v√≠deo") as pbar:
        frame_index = 0
        while cap.isOpened() and frame_index < total_frames:
            ret, frame = cap.read()
            if not ret or frame is None:
                break

            print(f"Processando frame {frame_index}...")
            try:
                analysis = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False, detector_backend='retinaface')
                if isinstance(analysis, list):
                    analysis = analysis[0]  # Se for uma lista, pegar o primeiro rosto detectado
                faces = [analysis] if isinstance(analysis, dict) else analysis
            except Exception as e:
                # Salvar o frame problem√°tico para inspe√ß√£o
                problem_frame_path = os.path.join(output_dir, f'problem_frame_{frame_index}.jpg')
                cv2.imwrite(problem_frame_path, frame)
                print(f"Frame problem√°tico salvo em: {problem_frame_path}")
                print(f"Erro ao analisar o frame {frame_index}: {e}")
                frame_index += 1
                continue

            if not faces:
                frame_index += 1
                continue

            current_frame_people = set()  # Pessoas detectadas neste frame
            for face in faces:
                if 'region' not in face:
                    continue
                box = face['region']
                x, y, w, h = box['x'], box['y'], box['w'], box['h']
                person_id = (x // position_threshold, y // position_threshold, w // position_threshold, h // position_threshold)  # Normalizando coordenadas para evitar m√∫ltiplas contagens do mesmo rosto
                current_frame_people.add(person_id)

                # Desenhar caixa delimitadora no rosto
                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
                label = f"Pessoa {len(detected_people_history) + 1}"
                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

            # Atualizar hist√≥rico e contar pessoas √∫nicas
            for person_id in current_frame_people:
                if person_id not in detected_people_history:
                    unique_people_count += 1
                    detected_people_history.add(person_id)

            out.write(frame)
            frame_index += 1
            pbar.update(1)

    cap.release()
    out.release()
    generate_faces_report(total_frames, unique_people_count)
    print(f"V√≠deo processado e salvo em: {output_video_path}")

# Fun√ß√£o para gerar relat√≥rio de reconhecimento facial
def generate_faces_report(total_frames, unique_people_count):
    report_path = os.path.join(output_dir, 'faces_recognition_report35e.txt')
    with open(report_path, 'w') as f:
        f.write("Resumo do Reconhecimento Facial\n\n")
        f.write(f"Total de frames analisados: {total_frames}\n")
        f.write(f"Total de pessoas √∫nicas detectadas (eventos √∫nicos): {unique_people_count}\n")
    print(f"Relat√≥rio gerado em: {report_path}")


# Caminho do v√≠deo de entrada
video_path = '/content/drive/MyDrive/Projeto_TechChallenge4/Unlocking Facial Recognition_ Diverse Activities Analysis.mp4'
process_video_faces_only(video_path)

"""## 2. üòä An√°lise de Express√µes Emocionais

## 3. üï∫ Detec√ß√£o de Atividades
Detecte e categorize as atividades sendo realizadas no v√≠deo.

Atividades a serem detectadas:*

  - ‚úã M√£o esquerda levantada

  - ‚úã M√£o direita levantada

  - üôå M√£os levantadas

  - üíÉ Dan√ßando

  - ü§ù Aperto de m√£os
  

## 4. üìÑ Gera√ß√£o de Resumo

Restringindo DAN√áA
shoulder_rotation:

A faixa foi ajustada de (0.5, 1.1) para (0.6, 1.0) para restringir o intervalo de rota√ß√£o dos ombros.
Isso reduz falsos positivos, garantindo que a rota√ß√£o detectada seja consistente com movimentos de dan√ßa.
shoulder_movement:

O limite foi aumentado de 0.25 para 0.3.
Isso significa que os ombros precisam se mover mais significativamente na vertical para serem considerados como parte da dan√ßa.
arm_movement:

O limite foi aumentado de 0.4 para 0.5.
Exige um movimento mais pronunciado dos bra√ßos em rela√ß√£o aos ombros para evitar a detec√ß√£o de movimentos menores como dan√ßa.
"""

import cv2
import mediapipe as mp
from fer import FER
from tqdm import tqdm
import os
import numpy as np
from PIL import Image, ImageDraw, ImageFont

# Inicializando MediaPipe Pose e FER
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)
mp_face = mp.solutions.face_mesh
face_mesh = mp_face.FaceMesh(static_image_mode=False, max_num_faces=6, min_detection_confidence=0.5)  #aumentando n¬∫ de faces em cada frame para 6, estava 1
fer_detector = FER(mtcnn=True)


# Configurando sa√≠da
output_dir = '/content/drive/MyDrive/Analise_Emocoes_TC4'
os.makedirs(output_dir, exist_ok=True)

# Hist√≥rico para anomalias
emotion_history = []
anomaly_threshold = 5  # Limite de mudan√ßas r√°pidas de emo√ß√µes para considerar anomalia

def detect_activities(pose_results):
    """
    Detecta atividades corporais como m√£os levantadas, dan√ßando e aperto de m√£os.
    """
    activities = []
    if pose_results.pose_landmarks:
        landmarks = pose_results.pose_landmarks.landmark
        left_hand = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]
        right_hand = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]
        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]
        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]
        nose = landmarks[mp_pose.PoseLandmark.NOSE]  #adicionando para separar melhor m√£os de rosto

        # Detectar m√£os levantadas com ajustes para n√£o detectar rosto como m√£o
        # Detectar m√£os levantadas - crit√©rios mais restritivos
        if (
            left_hand.y < left_shoulder.y and  # M√£o acima do ombro
            abs(left_hand.x - left_shoulder.x) > 0.1 and  # Longe do ombro lateralmente
            abs(left_hand.x - nose.x) > 0.15  # Longe do centro do rosto
        ):
            activities.append("Mao esquerda levantada")

        if (
            right_hand.y < right_shoulder.y and  # M√£o acima do ombro
            abs(right_hand.x - right_shoulder.x) > 0.1 and  # Longe do ombro lateralmente
            abs(right_hand.x - nose.x) > 0.15  # Longe do centro do rosto
        ):
            activities.append("Mao direita levantada")

        if (
            left_hand.y < left_shoulder.y and
            right_hand.y < right_shoulder.y and
            abs(left_hand.x - nose.x) > 0.15 and
            abs(right_hand.x - nose.x) > 0.15
        ):
            activities.append("Maos levantadas")

        # Detectar dan√ßa (movimento significativo do torso e ombros)
        shoulder_distance = abs(left_shoulder.x - right_shoulder.x)
        shoulder_movement = abs(left_shoulder.y - right_shoulder.y)
        arm_movement = max(abs(left_hand.x - left_shoulder.x), abs(right_hand.x - right_shoulder.x))
        shoulder_rotation = abs(left_shoulder.x - right_shoulder.x) / max(abs(left_shoulder.y - right_shoulder.y), 0.01)  # Evitar divis√£o por zero

        if (
            0.6 < shoulder_rotation < 1.0  # Crit√©rio mais restritivo para rota√ß√£o razo√°vel
            and (shoulder_movement > 0.3 or arm_movement > 0.5)  # Movimento significativo dos ombros ou bra√ßos
        ):
            activities.append("Dancando")

        # Detectar aperto de m√£os (m√£os pr√≥ximas e alinhadas)
        if (
            abs(left_hand.x - right_hand.x) < 0.015  # Ajustado para maior precis√£o no eixo X 0.015 pega uma s√≥ atividade
            and abs(left_hand.y - right_hand.y) < 0.015  # Ajustado para maior precis√£o no eixo Y
            and abs(left_hand.z - right_hand.z) < 0.015  # Inclu√≠do profundidade para maior robustez
        ):
            activities.append("Aperto de maos")

    return activities

def analyze_emotion(frame):
    """
    Analisa emo√ß√µes no frame usando FER e verifica por anomalias.
    """
    try:
        result = fer_detector.detect_emotions(frame)
        if result:
            emotions = result[0]['emotions']
            dominant_emotion = max(emotions, key=emotions.get)
            emotion_history.append(dominant_emotion)
            if len(emotion_history) > anomaly_threshold:
                emotion_history.pop(0)
                if len(set(emotion_history)) > 3:  # Muitas mudan√ßas r√°pidas
                    return "Anomaly Detected"
            return dominant_emotion
        else:
            return "Not Detected"
    except Exception as e:
        print(f"Erro ao analisar emo√ß√µes: {e}")
        return "Not Detected"

def add_text_with_background(frame, texts, font_size=1.0, font_thickness=2, bg_color=(200, 200, 200), text_color=(0, 0, 0), position=(10, 30), line_spacing=1.5):
    """
    Adiciona texto com fundo no frame usando OpenCV.
    """
    x, y = position
    for i, text in enumerate(texts):
        text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_size, font_thickness)
        text_width, text_height = text_size
        y_text = y + int(i * text_height * line_spacing)

        # Desenhar ret√¢ngulo de fundo
        cv2.rectangle(frame, (x, y_text - text_height - 5), (x + text_width + 10, y_text + 5), bg_color, -1)
        # Adicionar texto
        cv2.putText(frame, text, (x + 5, y_text), cv2.FONT_HERSHEY_SIMPLEX, font_size, text_color, font_thickness, cv2.LINE_AA)
    return frame

def process_video_emotions_activities(video_path):
    """
    Processa v√≠deo, analisando emo√ß√µes, atividades e detectando anomalias.
    """
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    output_video_path = os.path.join(output_dir, 'final91_video_emotions_activities.mp4')
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    events_summary = {
        'emotions': {},
        'activities': {},
        'unknown_emotions': 0,
        'anomalies': 0
    }

    last_emotion = None
    last_activities = set()

    with tqdm(total=total_frames, desc="Analisando v√≠deo") as pbar:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret or frame is None:
                break

            # Processando o frame para landmarks de pose
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pose_results = pose.process(frame_rgb)

            # Desenhando o esqueleto do Mediapipe no frame, se landmarks forem detectados
            if pose_results.pose_landmarks:
                mp.solutions.drawing_utils.draw_landmarks(
                    frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                    mp.solutions.drawing_utils.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),
                    mp.solutions.drawing_utils.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2)
                )

            # Analisar emo√ß√£o
            emotion = analyze_emotion(frame)
            if emotion == "Anomaly Detected":
                events_summary['anomalies'] += 1
            elif emotion == "Not Detected":
                events_summary['unknown_emotions'] += 1
            elif emotion != last_emotion:
                events_summary['emotions'][emotion] = events_summary['emotions'].get(emotion, 0) + 1
                last_emotion = emotion

            # Detectar atividades
            activities = set(detect_activities(pose_results))
            if activities != last_activities:
                for activity in activities:
                    events_summary['activities'][activity] = events_summary['activities'].get(activity, 0) + 1
                last_activities = activities

            # Adicionar legendas
            texts = [f"Emotion: {emotion}", f"Activities: {', '.join(activities) if activities else 'None'}"]
            frame = add_text_with_background(frame, texts, font_size=1.0, font_thickness=2, bg_color=(200, 200, 200), text_color=(0, 0, 0), position=(10, 30), line_spacing=2.0)

            # Escrever o frame no v√≠deo de sa√≠da
            out.write(frame)
            pbar.update(1)

    cap.release()
    out.release()
    print(f"V√≠deo processado e salvo em: {output_video_path}")

    generate_emotions_activities_report(total_frames, events_summary)


def generate_emotions_activities_report(total_frames, events_summary):
    """
    Gera relat√≥rio resumido de emo√ß√µes, atividades e anomalias.
    """
    report_path = os.path.join(output_dir, 'emotions_activities_report91.txt')
    with open(report_path, 'w') as f:
        f.write("Resumo de Emo√ß√µes, Atividades e Anomalias\n\n")
        f.write(f"Total de frames analisados: {total_frames}\n")
        f.write("Total de emo√ß√µes detectadas (eventos √∫nicos):\n")
        for emotion, count in events_summary['emotions'].items():
            f.write(f"  - {emotion}: {count}\n")
        f.write(f"Total de frames sem emo√ß√µes detectadas: {events_summary['unknown_emotions']}\n")
        f.write(f"Total de anomalias detectadas: {events_summary['anomalies']}\n")
        f.write("Total de atividades detectadas (eventos √∫nicos):\n")
        for activity, count in events_summary['activities'].items():
            f.write(f"  - {activity}: {count}\n")
    print(f"Relat√≥rio gerado em: {report_path}")

# Caminho do v√≠deo de entrada
video_path = '/content/drive/MyDrive/Analise_Emocoes_TC4/analisado35d_video_faces_only.mp4'
process_video_emotions_activities(video_path)

"""#Resumo de Emo√ß√µes, Atividades e Anomalias

###üìä Total de frames analisados: 1109

üòä Total de emo√ß√µes detectadas (eventos √∫nicos):
  - üòê Neutral: 33
  - üò¢ Sad: 24
  - üò° Angry: 17
  - üò® Fear: 13
  - üòÄ Happy: 23
  - üò≤ Surprise: 14

‚ùå Total de frames sem emo√ß√µes detectadas: 599

‚ö†Ô∏è Total de anomalias detectadas: 21

üèÉ‚Äç‚ôÇÔ∏è Total de atividades detectadas (eventos √∫nicos):
  - üíÉ Dan√ßando: 1
  - ‚úã M√£o esquerda levantada: 10
  - üôå M√£os levantadas: 5
  - ü§ö M√£o direita levantada: 6
  - ü§ù Aperto de m√£os: 1
#

## üé• Frames Totais
"""

import cv2
import mediapipe as mp
from fer import FER
from tqdm import tqdm
import os
import numpy as np
from PIL import Image, ImageDraw, ImageFont

# Inicializando MediaPipe Pose e FER
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)
mp_face = mp.solutions.face_mesh
face_mesh = mp_face.FaceMesh(static_image_mode=False, max_num_faces=6, min_detection_confidence=0.5)  #aumentando n¬∫ de faces em cada frame para 6, estava 1
fer_detector = FER(mtcnn=True)


# Configurando sa√≠da
output_dir = '/content/drive/MyDrive/Analise_Emocoes_TC4'
os.makedirs(output_dir, exist_ok=True)

# Hist√≥rico para anomalias
emotion_history = []
anomaly_threshold = 5  # Limite de mudan√ßas r√°pidas de emo√ß√µes para considerar anomalia

def detect_activities(pose_results):
    """
    Detecta atividades corporais como m√£os levantadas, dan√ßando e aperto de m√£os.
    """
    activities = []
    if pose_results.pose_landmarks:
        landmarks = pose_results.pose_landmarks.landmark
        left_hand = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]
        right_hand = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]
        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]
        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]
        nose = landmarks[mp_pose.PoseLandmark.NOSE]  #adicionando para separar melhor m√£os de rosto

        # Detectar m√£os levantadas com ajustes para n√£o detectar rosto como m√£o
        # Detectar m√£os levantadas - crit√©rios mais restritivos
        if (
            left_hand.y < left_shoulder.y and  # M√£o acima do ombro
            abs(left_hand.x - left_shoulder.x) > 0.1 and  # Longe do ombro lateralmente
            abs(left_hand.x - nose.x) > 0.15  # Longe do centro do rosto
        ):
            activities.append("Mao esquerda levantada")

        if (
            right_hand.y < right_shoulder.y and  # M√£o acima do ombro
            abs(right_hand.x - right_shoulder.x) > 0.1 and  # Longe do ombro lateralmente
            abs(right_hand.x - nose.x) > 0.15  # Longe do centro do rosto
        ):
            activities.append("Mao direita levantada")

        if (
            left_hand.y < left_shoulder.y and
            right_hand.y < right_shoulder.y and
            abs(left_hand.x - nose.x) > 0.15 and
            abs(right_hand.x - nose.x) > 0.15
        ):
            activities.append("Maos levantadas")

        # Detectar dan√ßa mais restritivo (movimento significativo do torso e ombros)
        shoulder_distance = abs(left_shoulder.x - right_shoulder.x)
        shoulder_movement = abs(left_shoulder.y - right_shoulder.y)
        arm_movement = max(abs(left_hand.x - left_shoulder.x), abs(right_hand.x - right_shoulder.x))
        shoulder_rotation = abs(left_shoulder.x - right_shoulder.x) / max(abs(left_shoulder.y - right_shoulder.y), 0.01)  # Evitar divis√£o por zero

        if (
            0.6 < shoulder_rotation < 1.0  # Crit√©rio mais restritivo para rota√ß√£o razo√°vel
            and (shoulder_movement > 0.3 or arm_movement > 0.5)  # Movimento significativo dos ombros ou bra√ßos
        ):
            activities.append("Dancando")

        # Detectar aperto de m√£os (m√£os pr√≥ximas e alinhadas)
        if (
            abs(left_hand.x - right_hand.x) < 0.015  # Ajustado para maior precis√£o no eixo X 0.015 pega uma s√≥ atividade
            and abs(left_hand.y - right_hand.y) < 0.015  # Ajustado para maior precis√£o no eixo Y
            and abs(left_hand.z - right_hand.z) < 0.015  # Inclu√≠do profundidade para maior robustez
        ):
            activities.append("Aperto de maos")

    return activities

def analyze_emotion(frame):
    """
    Analisa emo√ß√µes no frame usando FER e verifica por anomalias.
    """
    try:
        result = fer_detector.detect_emotions(frame)
        if result:
            emotions = result[0]['emotions']
            dominant_emotion = max(emotions, key=emotions.get)
            emotion_history.append(dominant_emotion)
            if len(emotion_history) > anomaly_threshold:
                emotion_history.pop(0)
                if len(set(emotion_history)) > 3:  # Muitas mudan√ßas r√°pidas
                    return "Anomaly Detected"
            return dominant_emotion
        else:
            return "Not Detected"
    except Exception as e:
        print(f"Erro ao analisar emo√ß√µes: {e}")
        return "Not Detected"

def add_text_with_background(frame, texts, font_size=1.0, font_thickness=2, bg_color=(200, 200, 200), text_color=(0, 0, 0), position=(10, 30), line_spacing=1.5):
    """
    Adiciona texto com fundo no frame usando OpenCV.
    """
    x, y = position
    for i, text in enumerate(texts):
        text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_size, font_thickness)
        text_width, text_height = text_size
        y_text = y + int(i * text_height * line_spacing)

        # Desenhar ret√¢ngulo de fundo
        cv2.rectangle(frame, (x, y_text - text_height - 5), (x + text_width + 10, y_text + 5), bg_color, -1)
        # Adicionar texto
        cv2.putText(frame, text, (x + 5, y_text), cv2.FONT_HERSHEY_SIMPLEX, font_size, text_color, font_thickness, cv2.LINE_AA)
    return frame

def process_video_emotions_activities(video_path):
    """
    Processa v√≠deo, analisando emo√ß√µes, atividades e detectando anomalias.
    """
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    output_video_path = os.path.join(output_dir, 'final92_video_emotions_activities.mp4')
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    events_summary = {
        'emotions': {},
        'activities': {},
        'unknown_emotions': 0,
        'anomalies': 0
    }

    last_emotion = None
    last_activities = set()

    with tqdm(total=total_frames, desc="Analisando v√≠deo") as pbar:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret or frame is None:
                break

            # Processando o frame para landmarks de pose
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pose_results = pose.process(frame_rgb)

            # Desenhando o esqueleto do Mediapipe no frame, se landmarks forem detectados
            if pose_results.pose_landmarks:
                mp.solutions.drawing_utils.draw_landmarks(
                    frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                    mp.solutions.drawing_utils.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),
                    mp.solutions.drawing_utils.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2)
                )

            # Analisar emo√ß√£o
            emotion = analyze_emotion(frame)
            if emotion == "Anomaly Detected":
                events_summary['anomalies'] += 1
            elif emotion == "Not Detected":
                events_summary['unknown_emotions'] += 1
            elif emotion != last_emotion:
                events_summary['emotions'][emotion] = events_summary['emotions'].get(emotion, 0) + 1
                last_emotion = emotion

            # Detectar atividades
            activities = set(detect_activities(pose_results))
            if activities != last_activities:
                for activity in activities:
                    events_summary['activities'][activity] = events_summary['activities'].get(activity, 0) + 1
                last_activities = activities

            # Adicionar legendas
            texts = [f"Emotion: {emotion}", f"Activities: {', '.join(activities) if activities else 'None'}"]
            frame = add_text_with_background(frame, texts, font_size=1.0, font_thickness=2, bg_color=(200, 200, 200), text_color=(0, 0, 0), position=(10, 30), line_spacing=2.0)

            # Escrever o frame no v√≠deo de sa√≠da
            out.write(frame)
            pbar.update(1)

    cap.release()
    out.release()
    print(f"V√≠deo processado e salvo em: {output_video_path}")

    generate_emotions_activities_report(total_frames, events_summary)


def generate_emotions_activities_report(total_frames, events_summary):
    """
    Gera relat√≥rio resumido de emo√ß√µes, atividades e anomalias.
    """
    report_path = os.path.join(output_dir, 'emotions_activities_report92.txt')
    with open(report_path, 'w') as f:
        f.write("Resumo de Emo√ß√µes, Atividades e Anomalias\n\n")
        f.write(f"Total de frames analisados: {total_frames}\n")
        f.write("Total de emo√ß√µes detectadas (eventos √∫nicos):\n")
        for emotion, count in events_summary['emotions'].items():
            f.write(f"  - {emotion}: {count}\n")
        f.write(f"Total de frames sem emo√ß√µes detectadas: {events_summary['unknown_emotions']}\n")
        f.write(f"Total de anomalias detectadas: {events_summary['anomalies']}\n")
        f.write("Total de atividades detectadas (eventos √∫nicos):\n")
        for activity, count in events_summary['activities'].items():
            f.write(f"  - {activity}: {count}\n")
    print(f"Relat√≥rio gerado em: {report_path}")

# Caminho do v√≠deo de entrada
video_path = '/content/drive/MyDrive/Analise_Emocoes_TC4/analisado35e_video_faces_only.mp4'
process_video_emotions_activities(video_path)

"""Resumo de Emo√ß√µes, Atividades e Anomalias

Total de frames analisados: 3326
Total de emo√ß√µes detectadas (eventos √∫nicos):
  - üòê neutral: 110
  - üò® fear: 49
  - üò¢ sad: 87
  - üò† angry: 31
  - üòä happy: 75
  - üò≤ surprise: 33
  - ü§¢ disgust: 1

Total de frames sem emo√ß√µes detectadas: 1761

Total de anomalias detectadas: 50

Total de atividades detectadas (eventos √∫nicos):
  - ‚úã (Esq) Mao esquerda levantada: 22
  - ‚úã (Dir) Mao direita levantada: 22
  - üôå Maos levantadas: 16
  - üíÉ Dancando: 2
  - ü§ù Aperto de maos: 1

Ajustes no c√≥digo:
1. Restringir emo√ß√µes repetidas:
Adicionei um controle temporal para que uma emo√ß√£o s√≥ seja detectada novamente ap√≥s um intervalo m√≠nimo de frames. Isso evita que a mesma emo√ß√£o seja contabilizada v√°rias vezes em sequ√™ncia.

2. Restringir falsos positivos de dan√ßa:
Atualizei os crit√©rios para incluir movimento consistente dos bra√ßos e ombros e um limite m√≠nimo para evitar que movimentos pequenos ou isolados sejam considerados dan√ßa.
"""

!pip install mediapipe
!pip install fer
!pip install tqdm
!pip install numpy
!pip install pillow
!pip install opencv-python

!pip uninstall mediapipe -y
!pip install mediapipe==0.10.9

!pip install --upgrade mediapipe

!pip install --upgrade pandas-gbq google-auth-oauthlib tensorflow

"""## üö´ Tirando Perfil da Detec√ß√£o de Anomalia"""

!pip install protobuf

"""## üõ†Ô∏è Ajustando Anomalias, Separando as Imagens

üìå Principais Ajustes:

   - ‚ö†Ô∏è Anomalias como Eventos √önicos:

    -- Verifica√ß√£o de frames consecutivos para eliminar duplica√ß√µes e manter apenas eventos realmente √∫nicos e relevantes.
"""

import cv2
import mediapipe as mp
from fer import FER
from tqdm import tqdm
import os
import numpy as np
from PIL import Image, ImageDraw, ImageFont

# Inicializando MediaPipe Pose e FaceDetection
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)
mp_face_detection = mp.solutions.face_detection
face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)
fer_detector = FER(mtcnn=True)

# Configurando sa√≠da
output_dir = '/content/drive/MyDrive/Analise_Emocoes_TC4'
os.makedirs(output_dir, exist_ok=True)

# Pasta para salvar frames de anomalias e atividades espec√≠ficas
anomaly_dir = os.path.join(output_dir, 'anomaly_frames20')
os.makedirs(anomaly_dir, exist_ok=True)

# Hist√≥rico para anomalias e atividades
emotion_history = []
anomaly_threshold = 7  # Ajustado para um hist√≥rico maior e mais seletivo

def detect_activities(pose_results):
    activities = []
    if pose_results.pose_landmarks:
        landmarks = pose_results.pose_landmarks.landmark
        left_hand = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]
        right_hand = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]
        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]
        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]
        nose = landmarks[mp_pose.PoseLandmark.NOSE]

        # Detectar m√£os levantadas
        if left_hand.y < left_shoulder.y and abs(left_hand.x - left_shoulder.x) > 0.1 and abs(left_hand.x - nose.x) > 0.15:
            activities.append("Mao esquerda levantada")
        if right_hand.y < right_shoulder.y and abs(right_hand.x - right_shoulder.x) > 0.1 and abs(right_hand.x - nose.x) > 0.15:
            activities.append("Mao direita levantada")
        if left_hand.y < left_shoulder.y and right_hand.y < right_shoulder.y and abs(left_hand.x - nose.x) > 0.15 and abs(right_hand.x - nose.x) > 0.15:
            activities.append("Maos levantadas")

        # Detectar dan√ßa (movimento significativo do torso e ombros)
        shoulder_rotation = abs(left_shoulder.x - right_shoulder.x) / max(abs(left_shoulder.y - right_shoulder.y), 0.01)
        shoulder_movement = abs(left_shoulder.y - right_shoulder.y)
        arm_movement = max(abs(left_hand.x - left_shoulder.x), abs(right_hand.x - right_shoulder.x))

        if 0.5 < shoulder_rotation < 1.1 and (shoulder_movement > 0.25 or arm_movement > 0.4):
            activities.append("Dancando")

        # Detectar aperto de m√£os (melhorado)
        if (
            left_hand.y > left_shoulder.y and right_hand.y > right_shoulder.y and  # M√£os abaixo dos ombros
            abs(left_hand.x - right_hand.x) < 0.015 and  # Ajuste horizontal mais permissivo
            abs(left_hand.y - right_hand.y) < 0.015 and  # Altura semelhante
            abs(left_hand.z - right_hand.z) < 0.015  # Profundidade realista
        ):
            activities.append("Aperto de maos")

    return activities

def analyze_emotion(frame):
    result = fer_detector.detect_emotions(frame)
    if result:
        emotions = result[0]['emotions']
        dominant_emotion = max(emotions, key=emotions.get)
        emotion_history.append(dominant_emotion)

        if len(emotion_history) > 15:
            emotion_history.pop(0)

        recent_changes = sum(1 for i in range(1, len(emotion_history)) if emotion_history[i] != emotion_history[i - 1])

        if recent_changes >= 8:
            return "Anomaly Detected"
        return dominant_emotion
    else:
        return "Not Detected"

def generate_emotions_activities_report(total_frames, events_summary):
    report_path = os.path.join(output_dir, 'emotions_activities_report_fdetection49.txt')

    emotion_icons = {"neutral": "üòê", "sad": "üò¢", "angry": "üò†", "fear": "üò®", "happy": "üòä", "surprise": "üò≤", "Not Detected": "‚ùì"}
    activity_icons = {"Mao esquerda levantada": "‚úã (Esq)", "Mao direita levantada": "‚úã (Dir)", "Maos levantadas": "üôå", "Dancando": "üíÉ", "Aperto de maos": "ü§ù"}

    with open(report_path, 'w') as f:
        f.write("# üìä Resumo de Emo√ß√µes, Atividades e Anomalias\n\n")
        f.write(f"**üé• Total de frames analisados:** {total_frames}\n\n")

        f.write("## üòä Emo√ß√µes detectadas (eventos √∫nicos):\n")
        for emotion, count in events_summary['emotions'].items():
            icon = emotion_icons.get(emotion, "‚ùì")
            f.write(f"- {icon} **{emotion.capitalize()}**: {count} vezes\n")

        # Agora escrevemos as anomalias uma √∫nica vez
        f.write(f"\n‚ö†Ô∏è **Total de Anomalias detectadas:** {len(events_summary['anomaly_frames'])}\n\n")

        f.write("## üîç Atividades detectadas (eventos √∫nicos):\n")
        for activity, count in events_summary['activities'].items():
            icon = activity_icons.get(activity, "üîπ")
            f.write(f"- {icon} **{activity.capitalize()}**: {count} vezes\n")

        f.write("\n## üì∏ Imagens de Anomalias e Atividades Destacadas:\n")
        for activity, frames in events_summary['activity_frames'].items():
            f.write(f"\n### üîç Atividade: **{activity}**\n")
            for frame in frames:
                f.write(f"- {frame}\n")

    print("\nüìä **Resumo do relat√≥rio:**")
    with open(report_path, 'r') as file:
        print(file.read())


def process_video_emotions_activities(video_path):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    output_video_path = os.path.join(output_dir, 'fdetection_video_emotions_activities49.mp4')
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    events_summary = {'emotions': {}, 'activities': {}, 'unknown_emotions': 0, 'anomalies': 0, 'anomaly_frames': [], 'activity_frames': {}}
    last_emotion = None
    last_activities = set()

    with tqdm(total=total_frames, desc="Analisando v√≠deo") as pbar:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret or frame is None:
                break

            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pose_results = pose.process(frame_rgb)
            face_results = face_detection.process(frame_rgb)

            if face_results.detections:
                emotion = analyze_emotion(frame)
            else:
                emotion = "Not Detected"

            if emotion == "Anomaly Detected":
                frame_name = f"anomaly_frame_{pbar.n}.jpg"
                cv2.imwrite(os.path.join(anomaly_dir, frame_name), frame)
                events_summary['anomaly_frames'].append(frame_name)
                events_summary['anomalies'] += 1  # Incrementa corretamente o contador

            if emotion != last_emotion:
                events_summary['emotions'][emotion] = events_summary['emotions'].get(emotion, 0) + 1
                last_emotion = emotion

            activities = set(detect_activities(pose_results))
            if activities != last_activities:
                for activity in activities:
                    events_summary['activities'][activity] = events_summary['activities'].get(activity, 0) + 1
                    if activity in ["Dancando", "Aperto de maos"]:
                        frame_name = f"{activity.lower().replace(' ', '_')}_frame_{pbar.n}.jpg"
                        cv2.imwrite(os.path.join(anomaly_dir, frame_name), frame)
                        events_summary['activity_frames'].setdefault(activity, []).append(frame_name)
                last_activities = activities

            out.write(frame)
            pbar.update(1)

    cap.release()
    out.release()
    print(f"üé¨ V√≠deo processado e salvo em: {output_video_path}")

    generate_emotions_activities_report(total_frames, events_summary)

# Caminho do v√≠deo de entrada
video_path = '/content/drive/MyDrive/Analise_Emocoes_TC4/analisado35e_video_faces_only.mp4'
process_video_emotions_activities(video_path)

"""## üì¶ Criando o Requirements"""

!pip freeze > requirements.txt
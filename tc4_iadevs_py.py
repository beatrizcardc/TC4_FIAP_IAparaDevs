# -*- coding: utf-8 -*-
"""TC4_IADevs.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X2NLUtdpbneCMYpFwwdOO1AQTq9kZazm

# ğŸŒŸ **Tech Challenge 4**

O **Tech Challenge** desta fase serÃ¡ a criaÃ§Ã£o de uma aplicaÃ§Ã£o que utilize **anÃ¡lise de vÃ­deo**.  

O projeto deve incorporar as seguintes tÃ©cnicas:  
ğŸ¯ **Reconhecimento facial**  
ğŸ­ **AnÃ¡lise de expressÃµes emocionais**  
ğŸƒâ€â™‚ï¸ **DetecÃ§Ã£o de atividades em vÃ­deos**  
ğŸ“Š **GeraÃ§Ã£o de resumo automÃ¡tico**

---

## ğŸš€ **A Proposta do Desafio**  

VocÃª deverÃ¡ criar uma aplicaÃ§Ã£o a partir do vÃ­deo disponÃ­vel na plataforma do aluno, que execute as seguintes tarefas:  

### 1. **ğŸ‘¤ Reconhecimento Facial**  
   - Identifique e marque os rostos presentes no vÃ­deo.  

### 2. **ğŸ˜Š AnÃ¡lise de ExpressÃµes Emocionais**  
   - Analise as expressÃµes emocionais dos rostos identificados.  

### 3. **ğŸ•º DetecÃ§Ã£o de Atividades**  
   - Detecte e categorize as atividades sendo realizadas no vÃ­deo.  

   **Atividades a serem detectadas:**  
   - âœ‹ **MÃ£o esquerda levantada**  
   - âœ‹ **MÃ£o direita levantada**  
   - ğŸ™Œ **MÃ£os levantadas**  
   - ğŸ’ƒ **DanÃ§ando**  
   - ğŸ¤ **Aperto de mÃ£os**  

### 4. **ğŸ“„ GeraÃ§Ã£o de Resumo**  
   - Crie um resumo automÃ¡tico das principais atividades e emoÃ§Ãµes detectadas no vÃ­deo.  
   - RelatÃ³rio gerado em formato `.txt` e exibido na tela com Ã­cones e informaÃ§Ãµes detalhadas.

---
"""

# 1. Instalar DeepFace e dependÃªncias essenciais
!pip install deepface opencv-python-headless retina-face

# 2. Atualizar pandas, tqdm, matplotlib para garantir compatibilidade
!pip install --upgrade pandas tqdm matplotlib

!pip install retina-face

!pip install mediapipe

!pip install deepface mediapipe moviepy tqdm opencv-python-headless

!pip install mtcnn

!pip install fer

!pip install tensorflow

#from google.colab import drive
#drive.mount('/content/drive')

!pip install tensorflow==2.9.1 keras==2.9.0
!pip install --upgrade deepface opencv-python-headless retina-face

"""Ajustando Reconhecimento Facial para precisÃ£o de evento Ãºnicos, sem pular Frames"""

!pip install --upgrade tensorflow deepface

!pip install tensorflow keras opencv-python-headless deepface

"""## 1. ğŸ‘¤ Reconhecimento Facial"""

import cv2
from deepface import DeepFace
from tqdm import tqdm
import os

# Configurando pasta de saÃ­da
output_dir = '/content/drive/MyDrive/Analise_Emocoes_TC4'
os.makedirs(output_dir, exist_ok=True)

# FunÃ§Ã£o principal para reconhecimento facial no vÃ­deo
def process_video_faces_only(video_path):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_fps = cap.get(cv2.CAP_PROP_FPS)
    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    output_video_path = os.path.join(output_dir, 'analisado35e_video_faces_only.mp4')
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, original_fps, (original_width, original_height))

    unique_people_count = 0
    detected_people_history = set()  # HistÃ³rico de pessoas detectadas
    frame_threshold = 10  # Limite de frames para considerar uma nova detecÃ§Ã£o
    position_threshold = 10  # Limite de variaÃ§Ã£o de posiÃ§Ã£o para considerar o mesmo rosto

    with tqdm(total=total_frames, desc="Analisando vÃ­deo") as pbar:
        frame_index = 0
        while cap.isOpened() and frame_index < total_frames:
            ret, frame = cap.read()
            if not ret or frame is None:
                break

            print(f"Processando frame {frame_index}...")
            try:
                analysis = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False, detector_backend='retinaface')
                if isinstance(analysis, list):
                    analysis = analysis[0]  # Se for uma lista, pegar o primeiro rosto detectado
                faces = [analysis] if isinstance(analysis, dict) else analysis
            except Exception as e:
                # Salvar o frame problemÃ¡tico para inspeÃ§Ã£o
                problem_frame_path = os.path.join(output_dir, f'problem_frame_{frame_index}.jpg')
                cv2.imwrite(problem_frame_path, frame)
                print(f"Frame problemÃ¡tico salvo em: {problem_frame_path}")
                print(f"Erro ao analisar o frame {frame_index}: {e}")
                frame_index += 1
                continue

            if not faces:
                frame_index += 1
                continue

            current_frame_people = set()  # Pessoas detectadas neste frame
            for face in faces:
                if 'region' not in face:
                    continue
                box = face['region']
                x, y, w, h = box['x'], box['y'], box['w'], box['h']
                person_id = (x // position_threshold, y // position_threshold, w // position_threshold, h // position_threshold)  # Normalizando coordenadas para evitar mÃºltiplas contagens do mesmo rosto
                current_frame_people.add(person_id)

                # Desenhar caixa delimitadora no rosto
                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
                label = f"Pessoa {len(detected_people_history) + 1}"
                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

            # Atualizar histÃ³rico e contar pessoas Ãºnicas
            for person_id in current_frame_people:
                if person_id not in detected_people_history:
                    unique_people_count += 1
                    detected_people_history.add(person_id)

            out.write(frame)
            frame_index += 1
            pbar.update(1)

    cap.release()
    out.release()
    generate_faces_report(total_frames, unique_people_count)
    print(f"VÃ­deo processado e salvo em: {output_video_path}")

# FunÃ§Ã£o para gerar relatÃ³rio de reconhecimento facial
def generate_faces_report(total_frames, unique_people_count):
    report_path = os.path.join(output_dir, 'faces_recognition_report35e.txt')
    with open(report_path, 'w') as f:
        f.write("Resumo do Reconhecimento Facial\n\n")
        f.write(f"Total de frames analisados: {total_frames}\n")
        f.write(f"Total de pessoas Ãºnicas detectadas (eventos Ãºnicos): {unique_people_count}\n")
    print(f"RelatÃ³rio gerado em: {report_path}")


# Caminho do vÃ­deo de entrada
video_path = '/content/drive/MyDrive/Projeto_TechChallenge4/Unlocking Facial Recognition_ Diverse Activities Analysis.mp4'
process_video_faces_only(video_path)

"""### AnÃ¡lise de EmoÃ§Ãµes, Atividades, Anomalias e RelatÃ³rio

### 2. ğŸ˜Š AnÃ¡lise de ExpressÃµes Emocionais

### 3. ğŸ•º DetecÃ§Ã£o de Atividades
Detecte e categorize as atividades sendo realizadas no vÃ­deo.

Atividades a serem detectadas:

  - âœ‹ MÃ£o esquerda levantada

  - âœ‹ MÃ£o direita levantada

  - ğŸ™Œ MÃ£os levantadas

  - ğŸ’ƒ DanÃ§ando

  - ğŸ¤ Aperto de mÃ£os
  

### 4. ğŸ“„ GeraÃ§Ã£o de Resumo

## ğŸ¥ Ajustes Finais

Ajustes no cÃ³digo:
1. ğŸ”’ Restringir emoÃ§Ãµes repetidas:

    --Adicionei um controle temporal para que uma emoÃ§Ã£o sÃ³ seja detectada novamente apÃ³s um intervalo mÃ­nimo de frames. Isso evita que a mesma emoÃ§Ã£o seja contabilizada vÃ¡rias vezes em sequÃªncia.


2. ğŸ”’ Restringir falsos positivos de danÃ§a:

    --Atualizei os critÃ©rios para incluir movimento consistente dos braÃ§os e ombros e um limite mÃ­nimo para evitar que movimentos pequenos ou isolados sejam considerados danÃ§a.


3. ğŸš« Tirando Perfil da DetecÃ§Ã£o de Anomalia

    -- Antes a detecÃ§Ã£o estava registrando alguns perfis como anomalias

4. âš ï¸ Anomalias como Eventos Ãšnicos:

    -- VerificaÃ§Ã£o de frames consecutivos para eliminar duplicaÃ§Ãµes e manter apenas eventos realmente Ãºnicos e relevantes.
"""

!pip install mediapipe
!pip install fer
!pip install tqdm
!pip install numpy
!pip install pillow
!pip install opencv-python

!pip uninstall mediapipe -y
!pip install mediapipe==0.10.9

!pip install --upgrade mediapipe

!pip install --upgrade pandas-gbq google-auth-oauthlib tensorflow

!pip install protobuf

import cv2
import mediapipe as mp
from fer import FER
from tqdm import tqdm
import os
import numpy as np
from PIL import Image, ImageDraw, ImageFont

# Inicializando MediaPipe Pose e FaceDetection
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)
mp_face_detection = mp.solutions.face_detection
face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)
fer_detector = FER(mtcnn=True)

# Configurando saÃ­da
output_dir = '/content/drive/MyDrive/Analise_Emocoes_TC4'
os.makedirs(output_dir, exist_ok=True)

# Pasta para salvar frames de anomalias e atividades especÃ­ficas
anomaly_dir = os.path.join(output_dir, 'anomaly_frames20')
os.makedirs(anomaly_dir, exist_ok=True)

# HistÃ³rico para anomalias e atividades
emotion_history = []
anomaly_threshold = 7  # Ajustado para um histÃ³rico maior e mais seletivo

def detect_activities(pose_results):
    activities = []
    if pose_results.pose_landmarks:
        landmarks = pose_results.pose_landmarks.landmark
        left_hand = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]
        right_hand = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]
        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]
        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]
        nose = landmarks[mp_pose.PoseLandmark.NOSE]

        # Detectar mÃ£os levantadas
        if left_hand.y < left_shoulder.y and abs(left_hand.x - left_shoulder.x) > 0.1 and abs(left_hand.x - nose.x) > 0.15:
            activities.append("Mao esquerda levantada")
        if right_hand.y < right_shoulder.y and abs(right_hand.x - right_shoulder.x) > 0.1 and abs(right_hand.x - nose.x) > 0.15:
            activities.append("Mao direita levantada")
        if left_hand.y < left_shoulder.y and right_hand.y < right_shoulder.y and abs(left_hand.x - nose.x) > 0.15 and abs(right_hand.x - nose.x) > 0.15:
            activities.append("Maos levantadas")

        # Detectar danÃ§a (movimento significativo do torso e ombros)
        shoulder_rotation = abs(left_shoulder.x - right_shoulder.x) / max(abs(left_shoulder.y - right_shoulder.y), 0.01)
        shoulder_movement = abs(left_shoulder.y - right_shoulder.y)
        arm_movement = max(abs(left_hand.x - left_shoulder.x), abs(right_hand.x - right_shoulder.x))

        if 0.5 < shoulder_rotation < 1.1 and (shoulder_movement > 0.25 or arm_movement > 0.4):
            activities.append("Dancando")

        # Detectar aperto de mÃ£os (melhorado)
        if (
            left_hand.y > left_shoulder.y and right_hand.y > right_shoulder.y and  # MÃ£os abaixo dos ombros
            abs(left_hand.x - right_hand.x) < 0.015 and  # Ajuste horizontal mais permissivo
            abs(left_hand.y - right_hand.y) < 0.015 and  # Altura semelhante
            abs(left_hand.z - right_hand.z) < 0.015  # Profundidade realista
        ):
            activities.append("Aperto de maos")

    return activities

def analyze_emotion(frame):
    result = fer_detector.detect_emotions(frame)
    if result:
        emotions = result[0]['emotions']
        dominant_emotion = max(emotions, key=emotions.get)
        emotion_history.append(dominant_emotion)

        if len(emotion_history) > 15:
            emotion_history.pop(0)

        recent_changes = sum(1 for i in range(1, len(emotion_history)) if emotion_history[i] != emotion_history[i - 1])

        if recent_changes >= 8:
            return "Anomaly Detected"
        return dominant_emotion
    else:
        return "Not Detected"

def generate_emotions_activities_report(total_frames, events_summary):
    report_path = os.path.join(output_dir, 'emotions_activities_report_fdetection49.txt')

    emotion_icons = {"neutral": "ğŸ˜", "sad": "ğŸ˜¢", "angry": "ğŸ˜ ", "fear": "ğŸ˜¨", "happy": "ğŸ˜Š", "surprise": "ğŸ˜²", "Not Detected": "â“"}
    activity_icons = {"Mao esquerda levantada": "âœ‹ (Esq)", "Mao direita levantada": "âœ‹ (Dir)", "Maos levantadas": "ğŸ™Œ", "Dancando": "ğŸ’ƒ", "Aperto de maos": "ğŸ¤"}

    with open(report_path, 'w') as f:
        f.write("# ğŸ“Š Resumo de EmoÃ§Ãµes, Atividades e Anomalias\n\n")
        f.write(f"**ğŸ¥ Total de frames analisados:** {total_frames}\n\n")

        f.write("## ğŸ˜Š EmoÃ§Ãµes detectadas (eventos Ãºnicos):\n")
        for emotion, count in events_summary['emotions'].items():
            icon = emotion_icons.get(emotion, "â“")
            f.write(f"- {icon} **{emotion.capitalize()}**: {count} vezes\n")

        # Agora escrevemos as anomalias uma Ãºnica vez
        f.write(f"\nâš ï¸ **Total de Anomalias detectadas:** {len(events_summary['anomaly_frames'])}\n\n")

        f.write("## ğŸ” Atividades detectadas (eventos Ãºnicos):\n")
        for activity, count in events_summary['activities'].items():
            icon = activity_icons.get(activity, "ğŸ”¹")
            f.write(f"- {icon} **{activity.capitalize()}**: {count} vezes\n")

        f.write("\n## ğŸ“¸ Imagens de Anomalias e Atividades Destacadas:\n")
        for activity, frames in events_summary['activity_frames'].items():
            f.write(f"\n### ğŸ” Atividade: **{activity}**\n")
            for frame in frames:
                f.write(f"- {frame}\n")

    print("\nğŸ“Š **Resumo do relatÃ³rio:**")
    with open(report_path, 'r') as file:
        print(file.read())


def process_video_emotions_activities(video_path):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    output_video_path = os.path.join(output_dir, 'fdetection_video_emotions_activities49.mp4')
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    events_summary = {'emotions': {}, 'activities': {}, 'unknown_emotions': 0, 'anomalies': 0, 'anomaly_frames': [], 'activity_frames': {}}
    last_emotion = None
    last_activities = set()

    with tqdm(total=total_frames, desc="Analisando vÃ­deo") as pbar:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret or frame is None:
                break

            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pose_results = pose.process(frame_rgb)
            face_results = face_detection.process(frame_rgb)

            if face_results.detections:
                emotion = analyze_emotion(frame)
            else:
                emotion = "Not Detected"

            if emotion == "Anomaly Detected":
                frame_name = f"anomaly_frame_{pbar.n}.jpg"
                cv2.imwrite(os.path.join(anomaly_dir, frame_name), frame)
                events_summary['anomaly_frames'].append(frame_name)
                events_summary['anomalies'] += 1  # Incrementa corretamente o contador

            if emotion != last_emotion:
                events_summary['emotions'][emotion] = events_summary['emotions'].get(emotion, 0) + 1
                last_emotion = emotion

            activities = set(detect_activities(pose_results))
            if activities != last_activities:
                for activity in activities:
                    events_summary['activities'][activity] = events_summary['activities'].get(activity, 0) + 1
                    if activity in ["Dancando", "Aperto de maos"]:
                        frame_name = f"{activity.lower().replace(' ', '_')}_frame_{pbar.n}.jpg"
                        cv2.imwrite(os.path.join(anomaly_dir, frame_name), frame)
                        events_summary['activity_frames'].setdefault(activity, []).append(frame_name)
                last_activities = activities

            out.write(frame)
            pbar.update(1)

    cap.release()
    out.release()
    print(f"ğŸ¬ VÃ­deo processado e salvo em: {output_video_path}")

    generate_emotions_activities_report(total_frames, events_summary)

# Caminho do vÃ­deo de entrada
video_path = '/content/drive/MyDrive/Analise_Emocoes_TC4/analisado35e_video_faces_only.mp4'
process_video_emotions_activities(video_path)

"""## ğŸ“¦ Criando o Requirements"""

!pip freeze > requirements.txt